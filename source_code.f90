module agent_class
    implicit none

    type,public::Classical_Agent   
    integer::Action        !The action from chosen policy  
    integer,allocatable::StrategyBase(:,:)   !  the strategy base consisit of matrix 
    integer(kind = 8),allocatable::VirtualScore(:)    ! the virtual score of each strategy base 
    ! 1:2 = kind_policy; 0:7: all states    
    real(kind = 8)::Reward               ! the reward of each classical agents
    end type Classical_Agent

    type,public::Q_Learning_Agent
    integer::Action                         
    real(kind=8),allocatable::Qtable(:,:)          ! the Q-table of Q-agents
    real(kind=8)::Reward                 ! the reward of each Q-agents
    end type Q_Learning_Agent

end module agent_class


module initialization
    use agent_class
    implicit none

    contains
    subroutine initialize_histroy(MemoryLength, History)
        implicit none
        integer,intent(in)::MemoryLength
        integer,allocatable::History(:)
        real(kind = 8),allocatable::ran(:)
        allocate(ran(MemoryLength))
        call random_number(ran)
        History = merge(0, 1, ran.lt.0.5)                           !initialize a random history
        deallocate(ran)
        return
    end subroutine initialize_histroy

    subroutine initialize_classical_agent(Agent1, Num1, NumberMemoryKind, NumberStrategies)
        implicit none
        type(Classical_Agent),allocatable::Agent1(:)             ! the classical agents             
        integer,intent(in)::Num1                                 ! the number of classical agents
        integer,intent(in)::NumberStrategies, NumberMemoryKind     ! memory_length: m      NumberMemoryKind: the number of possible histories        
        real(kind = 8),allocatable::ran(:)                   
        integer::i, j, k, size1, temp
        real(kind = 8),allocatable:: ran_array(:)
        integer::duplicate_result
        logical :: has_duplicates
        allocate(ran_array(NumberMemoryKind))

        do i = 1, Num1, 1 
            Agent1(i)%StrategyBase = 0
            do j = 1, NumberStrategies, 1
                call random_number(ran_array);
                Agent1(i)%StrategyBase(:, j) = merge(0, 1, ran_array.le.0.5)             !generating the jth strategy
                has_duplicates = .false.
                if(j.ge.2)then
                    do k = 1, j - 1, 1
                        if(sum(Agent1(i)%StrategyBase(:,j) - Agent1(i)%StrategyBase(:,k)).eq.0)then       
                            has_duplicates = .true.                                      !the jth strategy is identcal to the kth strategy             
                            exit
                        end if
                    end do
                end if
                if(has_duplicates)then
                    cycle
                end if
            end do
            Agent1(i)%VirtualScore  = 0                   !initialize the VirtualScore as zero
        end do 
        deallocate(ran_array);
        return
    end subroutine initialize_classical_agent

    subroutine initialize_Qlerning_agent(Agent2, Num2)
        implicit none
        type(Q_Learning_Agent),allocatable::Agent2(:)    ! the Reinforcement_Learning_Agent
        integer,intent(in)::Num2                                   ! the number of the Q-Learning agents
        integer::i
        do i = 1, Num2, 1
            call random_number(Agent2(i)%Qtable)
            Agent2(i)%QTable = (Agent2(i)%Qtable - 0.5)*0.01           !generating Q-tables for Q-Learning agents
        end do
        return
    end subroutine initialize_Qlerning_agent

end module initialization

module update_process
use agent_class
    implicit none
    contains
    subroutine update(Agent1, Agent2, Num1, Num2, NumberStrategies, ResourceCapacity, &
        NumberMemoryKind, MemoryLength, History, Alpha, Gamma, Epsilon, TotalAction)
        type(Classical_Agent),allocatable::Agent1(:)                     !classical agents
        type(Q_Learning_Agent),allocatable::Agent2(:)                    !Q-learning agents
        integer,intent(in)::Num1,Num2                                    ! the number of classical agents and Q-learning agents    
        integer,intent(in)::MemoryLength, NumberStrategies, NumberMemoryKind       
        integer,allocatable::History(:), NewHistory(:)                  ! the history of winning resource and new hisory after the current round          
        real(kind = 8),intent(in)::Alpha, Gamma, Epsilon                 ! the learning parameters for Q-learning agents    
        integer::ResourceCapacity(2)                                    ! the capacities of the two resource
        integer::State, NewState                                        ! the state of current round and next round, which are generated by the history and new hisory
        integer::i, j, k, i1, j1, k1, size1                              ! cycle indix
        integer::max_score                                               ! the maximum virtual score of the strategy in base   
        integer,allocatable::max_indices(:)                              ! the indices of strategies which has the maximum virtual score
        integer::count, index1, numbering
        integer,allocatable::score_array(:)                              ! temptation variables for saving the virtual score
        real(kind = 8)::ran1, ran2
        integer::TotalAction, WinningAction                                ! total_action:the number of agents selecting resource 1; win_action: the winning action in the current round 
        size1 = size(History, 1); allocate(NewHistory(size1))
        State = 0
        do i = 1, size1, 1
            State = State + History(size1-i+1)*2**(i-1)                           ! generating state according to history
        end do
        allocate(score_array(NumberStrategies))  !from zero to number_stragegies
        TotalAction = 0;
        do i = 1, Num1, 1
            count = 0; score_array = Agent1(i)%VirtualScore  
            max_score = maxval(score_array)   
            do j = 1, size(score_array), 1
                if(score_array(j) == max_score) then
                    count = count + 1
                end if
            end do                  
            !get the maximum score  and the "count" record the number of strategies which have the maximum score 
            allocate(max_indices(count))
            max_indices = 0
            index1 = 1
            do j = 1, size(score_array), 1
                if(score_array(j) == max_score) then
                    max_indices(index1) = j
                    index1 = index1 + 1
                end if
            end do                   ! get the all indices of stratigies with the maximum score   
            call random_number(ran1)
            numbering = max_indices(int(ran1*count) + 1)    !randomly select one strategy which has the maximum virtual score
            !get one number of one strategies that has the maxmum score
            Agent1(i)%Action = Agent1(i)%StrategyBase(State, numbering)
            TotalAction = TotalAction + Agent1(i)%Action
            deallocate(max_indices);
        end do
        do i = 1, Num2, 1
            call random_number(ran1)
            if(ran1.lt.Epsilon)then
                call random_number(ran2)
                Agent2(i)%Action = merge(0, 1, ran2.lt.0.5)                      ! take an action randomly with probability Epsilon
            else
                Agent2(i)%Action = merge(0, 1, Agent2(i)%Qtable(State, 0).gt.Agent2(i)%Qtable(State, 1))                 ! take the action according to Q-table with probability (1-Epsilon)
            end if
            TotalAction = TotalAction + Agent2(i)%Action
        end do  

        WinningAction = merge(1, 0, TotalAction.le.ResourceCapacity(1))              ! get the winning action in the current round

        !generating new history
        NewHistory(1:size(history, 1)-1) = History(2:size(History, 1)); !shift history                
        NewHistory(size(History)) = WinningAction                  !add new histroy
        ! generating new state
        NewState = 0                            
        do i = 1, size1, 1
            NewState = NewState + NewHistory(size1-i+1)*2**(i-1)                  
        end do

        !get reward of classical agents and VirtualScore of classical agents
        do i = 1, Num1, 1
            Agent1(i)%reward = merge(1, -1, Agent1(i)%Action.eq.WinningAction)

            do j = 1, NumberStrategies, 1
                Agent1(i)%VirtualScore(j) = merge(Agent1(i)%VirtualScore(j) + 1,  Agent1(i)%VirtualScore(j) - 1, &
                    Agent1(i)%StrategyBase(State, j).eq.WinningAction)
            end do
        end do

        !update Q-table for all agents2 according to their reward 
        do i = 1, Num2, 1
            Agent2(i)%Reward = merge(1, -1, Agent2(i)%Action.eq.WinningAction)                  
            if(Agent2(i)%Qtable(NewState, 0).gt.Agent2(i)%Qtable(NewState, 1))then
                Agent2(i)%Qtable(State, Agent2(i)%Action) =  Agent2(i)%Qtable(State, Agent2(i)%action)*(1-Alpha)+&
                Alpha*(Gamma*Agent2(i)%Qtable(NewState, 0) + Agent2(i)%Reward)
            else
                Agent2(i)%Qtable(State, Agent2(i)%Action) =  Agent2(i)%Qtable(State, Agent2(i)%action)*(1-Alpha)+&
                Alpha*(Gamma*Agent2(i)%Qtable(NewState, 1) + Agent2(i)%Reward)
            end if
        end do
        History = NewHistory
        deallocate(NewHistory); deallocate(score_array);
        return
    end subroutine update   

end module update_process

program main
    use agent_class
    use initialization
    use update_process
    implicit none
    integer:: NumberMemoryKind, NumberStrategies, MemoryLength
    type(Classical_Agent),allocatable::Agent1(:)
    type(Q_Learning_Agent),allocatable::Agent2(:)
    real(kind = 8)::Alpha, Gamma, Epsilon               !learning parameters of Q-learning agents
    integer,allocatable::History(:)                     ! the history of winning resource in the current round
    integer::State                                      ! the state generated by history
    integer::Num1, Num2, Num                            !num1(num2): the number of classical agents(Q-agents); num:the total number of agents
    real(kind = 8)::Fraction1                           ! the fraction of classical agents(fc in paper)
    integer::ResourceCapacity(2)                        ! The capacities of two resources. 
    integer::i, j, k, i1, j1, k1, t, seed1(33)
    integer(kind = 8)::TransientTime = 20000000, DesiredTime = 2000000                
    real(kind = 8)::Psi                                        ! the volatility of the population 
    integer::TotalAction,run                                        ! the number of agents selecting resource 1
    integer::fluctuation_2                                        ! the square of fluctuation of the population around capacity
    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    open(unit =100, file = 'result.txt')

    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    MemoryLength = 3;
    NumberMemoryKind = 2**MemoryLength; 
    NumberStrategies = 2; Allocate(History(MemoryLength))          !the number of strategies in base is 2 and 3 rounds history will be remembered
    !!!!!!!!!!!!!!!!!!!!!!!!!!
    Alpha = 0.1; Gamma = 0.9; Epsilon = 0.01;
    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    Num = 301;    
       
    do run =1,10 
        do i = 1, 33, 1
                seed1(i) = 1654687 + i
        end do
        call random_seed(put = seed1)
        do i1 = 0, 100
            Fraction1 =i1 * 0.01
            ResourceCapacity = int(Num/2);                        !The capacities for both resource are (Num-1)/2 
            Num1 = int(Num*Fraction1); Num2 = Num - Num1
            allocate(Agent1(Num1));            
            allocate(Agent2(Num2)); 
            do i = 1, num1, 1
                allocate(Agent1(i)%VirtualScore(NumberStrategies))
                allocate(Agent1(i)%StrategyBase(0:NumberMemoryKind-1, NumberStrategies))
            end do
            do i = 1, num2, 1
                allocate(Agent2(i)%Qtable(0:NumberMemoryKind-1, 0:1))
            end do
            call initialize_histroy(MemoryLength, History)
            call initialize_classical_agent(Agent1, Num1, NumberMemoryKind, NumberStrategies)
            call initialize_Qlerning_agent(Agent2, Num2)
            do t = 1, TransientTime, 1
                call update(Agent1, Agent2, Num1, Num2, NumberStrategies, ResourceCapacity, &
                NumberMemoryKind, MemoryLength, History, Alpha, Gamma, Epsilon, TotalAction)                      
            end do
            fluctuation_2 = 0
            do t = 1, DesiredTime, 1
                call update(Agent1, Agent2, Num1, Num2, NumberStrategies, ResourceCapacity, &
                NumberMemoryKind, MemoryLength, History, Alpha, Gamma, Epsilon, TotalAction)
                fluctuation_2 = (TotalAction - ResourceCapacity(1))**2 + fluctuation_2
            end do
            Psi = real(fluctuation_2)/real(Num*DesiredTime)
            write(100,'(F6.3,1X,E15.7)') Fraction1, Psi
            do i = 1, Num1, 1
                deallocate(Agent1(i)%VirtualScore, Agent1(i)%StrategyBase)
            end do
            do i = 1, Num2, 1
                deallocate(Agent2(i)%Qtable)
            end do
            deallocate(Agent1, Agent2)
        end do
    end do
end program 



    

    
